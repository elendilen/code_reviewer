"""
Optimization Advisor Agent - ä¼˜åŒ–é¡¾é—®
æ±‡æ€»æ‰€æœ‰åˆ†æç»“æœï¼Œç»™å‡ºå…·ä½“å¯è¡Œçš„ä¼˜åŒ–æ–¹æ¡ˆ
"""
from typing import List, Dict, Any
from langchain_ollama import ChatOllama
from langchain_core.messages import HumanMessage
from .perf_state import (PerformanceState, OptimizationSuggestion, HotspotInfo,
                         MemoryIssue)
from ..utils.logger import setup_logger

logger = setup_logger("optimization_advisor")


# ä¼˜åŒ–æ¨¡å¼çŸ¥è¯†åº“
OPTIMIZATION_PATTERNS = {
    "algorithm_replacement": {
        "linear_to_binary": {
            "problem": "çº¿æ€§æœç´¢ O(n)",
            "solution": "ä½¿ç”¨äºŒåˆ†æœç´¢ O(log n)ï¼ˆéœ€è¦æ•°æ®æœ‰åºï¼‰",
            "condition": "æ•°æ®å·²æ’åºæˆ–å¯ä»¥æ’åº"
        },
        "bubble_to_quick": {
            "problem": "å†’æ³¡æ’åº O(nÂ²)",
            "solution": "ä½¿ç”¨å¿«é€Ÿæ’åº O(n log n)",
            "condition": "é€šç”¨åœºæ™¯"
        },
        "list_to_hashmap": {
            "problem": "é“¾è¡¨æŸ¥æ‰¾ O(n)",
            "solution": "ä½¿ç”¨å“ˆå¸Œè¡¨ O(1)",
            "condition": "éœ€è¦é¢‘ç¹æŸ¥æ‰¾"
        },
        "array_to_heap": {
            "problem": "æ•°ç»„æ‰¾æœ€å€¼ O(n)",
            "solution": "ä½¿ç”¨å † O(log n)",
            "condition": "é¢‘ç¹è·å–æœ€å¤§/æœ€å°å€¼"
        }
    },
    "loop_optimization": {
        "loop_unrolling": {
            "problem": "å¾ªç¯å¼€é”€å¤§",
            "solution": "å¾ªç¯å±•å¼€",
            "code_example": "// å±•å¼€å‰\nfor(i=0;i<n;i++) a[i]=0;\n// å±•å¼€å\nfor(i=0;i<n;i+=4) {a[i]=0;a[i+1]=0;a[i+2]=0;a[i+3]=0;}"
        },
        "loop_fusion": {
            "problem": "å¤šä¸ªå¾ªç¯éå†åŒä¸€æ•°ç»„",
            "solution": "åˆå¹¶å¾ªç¯",
        },
        "loop_invariant": {
            "problem": "å¾ªç¯å†…é‡å¤è®¡ç®—",
            "solution": "å°†ä¸å˜é‡ç§»åˆ°å¾ªç¯å¤–",
        }
    },
    "memory_optimization": {
        "object_pool": {
            "problem": "é¢‘ç¹åˆ†é…/é‡Šæ”¾å°å¯¹è±¡",
            "solution": "ä½¿ç”¨å¯¹è±¡æ± ",
        },
        "preallocate": {
            "problem": "åŠ¨æ€å¢é•¿çš„æ•°ç»„",
            "solution": "é¢„åˆ†é…è¶³å¤Ÿç©ºé—´",
        },
        "cache_friendly": {
            "problem": "ç¼“å­˜ä¸å‹å¥½çš„è®¿é—®æ¨¡å¼",
            "solution": "æ”¹ä¸ºè¿ç»­å†…å­˜è®¿é—®",
        }
    },
    "parallelization": {
        "parallel_loop": {
            "problem": "å¯å¹¶è¡Œçš„ç‹¬ç«‹å¾ªç¯",
            "solution": "ä½¿ç”¨ OpenMP æˆ–çº¿ç¨‹æ± å¹¶è¡ŒåŒ–",
        },
        "async_io": {
            "problem": "åŒæ­¥ I/O é˜»å¡",
            "solution": "ä½¿ç”¨å¼‚æ­¥ I/O",
        }
    }
}


class OptimizationAdvisorAgent:
    """ä¼˜åŒ–é¡¾é—® Agent"""
    
    def __init__(self, model_name="qwen2.5-coder:7b", base_url="http://localhost:11434"):
        self.llm = ChatOllama(model=model_name, base_url=base_url, temperature=0.1)
    
    def advise(self, state: PerformanceState) -> Dict[str, Any]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        functions = state["functions"]
        hotspots = state.get("hotspots", [])
        memory_issues = state.get("memory_issues", [])
        language = state.get("language", "c")
        
        logger.info(f"å¼€å§‹ç”Ÿæˆä¼˜åŒ–å»ºè®®ï¼Œ{len(hotspots)} ä¸ªçƒ­ç‚¹")
        
        optimizations: List[OptimizationSuggestion] = []
        
        # 1. åŸºäºçƒ­ç‚¹ç”Ÿæˆä¼˜åŒ–å»ºè®®
        for hotspot in hotspots[:5]:
            suggestions = self._generate_hotspot_suggestions(
                hotspot, functions, language
            )
            optimizations.extend(suggestions)
        
        # 2. åŸºäºå†…å­˜é—®é¢˜ç”Ÿæˆä¼˜åŒ–å»ºè®®
        memory_suggestions = self._generate_memory_suggestions(memory_issues, language)
        optimizations.extend(memory_suggestions)
        
        # 3. ä½¿ç”¨ LLM ç”Ÿæˆç»¼åˆä¼˜åŒ–æŠ¥å‘Š
        if hotspots:
            llm_suggestions = self._llm_optimization_analysis(
                state, functions, hotspots, memory_issues, language
            )
            optimizations.extend(llm_suggestions)
        
        # 4. å»é‡å’Œæ’åº
        optimizations = self._deduplicate_and_prioritize(optimizations)
        
        # 5. ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
        performance_report = self._generate_performance_report(
            state, optimizations
        )
        
        logger.info(f"ç”Ÿæˆäº† {len(optimizations)} æ¡ä¼˜åŒ–å»ºè®®")
        
        return {
            "optimizations": optimizations,
            "performance_report": performance_report
        }
    
    def _generate_hotspot_suggestions(self, hotspot: HotspotInfo,
                                       functions: List,
                                       language: str) -> List[OptimizationSuggestion]:
        """ä¸ºçƒ­ç‚¹ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        suggestions = []
        func_name = hotspot["function"]
        
        # æ‰¾åˆ°å¯¹åº”çš„å‡½æ•°ä¿¡æ¯
        func_info = next((f for f in functions if f["name"] == func_name), None)
        
        if not func_info:
            return suggestions
        
        # æ£€æŸ¥å¾ªç¯ä¼˜åŒ–æœºä¼š
        loops = func_info.get("loops", [])
        if len(loops) >= 2:
            suggestions.append(OptimizationSuggestion(
                target=func_name,
                priority="medium",
                category="loop",
                problem=f"å‡½æ•°åŒ…å« {len(loops)} ä¸ªå¾ªç¯ï¼Œå¯èƒ½å­˜åœ¨ä¼˜åŒ–ç©ºé—´",
                solution="æ£€æŸ¥æ˜¯å¦å¯ä»¥åˆå¹¶å¾ªç¯ï¼ˆloop fusionï¼‰æˆ–å°†å¾ªç¯ä¸å˜é‡ç§»å‡ºå¾ªç¯",
                code_before="",
                code_after="",
                expected_improvement="å‡å°‘å¾ªç¯å¼€é”€å’Œå†…å­˜è®¿é—®"
            ))

        # é«˜è°ƒç”¨æ‰‡å‡ºï¼šè€ƒè™‘ç¼“å­˜/æ‰¹å¤„ç†/å‡å°‘è·¨å±‚è°ƒç”¨
        calls = func_info.get("calls", [])
        if len(calls) >= 8:
            suggestions.append(OptimizationSuggestion(
                target=func_name,
                priority="medium",
                category="cache",
                problem=f"å‡½æ•°è°ƒç”¨å…¶ä»–å‡½æ•°è¾ƒå¤šï¼ˆ{len(calls)} ä¸ªï¼‰ï¼Œå¯èƒ½å­˜åœ¨é¢‘ç¹å°è°ƒç”¨å¼€é”€/é‡å¤è®¡ç®—",
                solution="æ£€æŸ¥æ˜¯å¦å­˜åœ¨å¯ç¼“å­˜çš„ä¸­é—´ç»“æœï¼›å°†ç»†ç²’åº¦è°ƒç”¨åˆå¹¶ä¸ºæ‰¹å¤„ç†ï¼›å‡å°‘é‡å¤çš„è¾¹ç•Œæ£€æŸ¥ä¸æ—¥å¿—",
                code_before="",
                code_after="",
                expected_improvement="å‡å°‘å‡½æ•°è°ƒç”¨å¼€é”€ä¸é‡å¤è®¡ç®—"
            ))
        
        return suggestions
    
    def _generate_memory_suggestions(self, memory_issues: List[MemoryIssue],
                                      language: str) -> List[OptimizationSuggestion]:
        """åŸºäºå†…å­˜é—®é¢˜ç”Ÿæˆå»ºè®®"""
        suggestions = []
        
        for issue in memory_issues[:5]:
            if issue["type"] == "potential_leak":
                suggestions.append(OptimizationSuggestion(
                    target=f"{issue['file']}:{issue['line']}",
                    priority="high",
                    category="memory",
                    problem=issue["description"],
                    solution=issue["suggestion"],
                    code_before="",
                    code_after="",
                    expected_improvement="æ¶ˆé™¤å†…å­˜æ³„æ¼"
                ))
            elif issue["type"] == "missing_null_check":
                suggestions.append(OptimizationSuggestion(
                    target=f"{issue['file']}:{issue['line']}",
                    priority="medium",
                    category="memory",
                    problem=issue["description"],
                    solution=issue["suggestion"],
                    code_before="ptr = malloc(size);\nuse(ptr);",
                    code_after="ptr = malloc(size);\nif(ptr == NULL) { /* handle error */ }\nuse(ptr);",
                    expected_improvement="æé«˜ä»£ç å¥å£®æ€§"
                ))
        
        return suggestions
    
    def _llm_optimization_analysis(self, state: PerformanceState,
                                   functions: List,
                                   hotspots: List[HotspotInfo],
                                   memory_issues: List,
                                   language: str) -> List[OptimizationSuggestion]:
        """ä½¿ç”¨ LLM ç”Ÿæˆè¯¦ç»†ä¼˜åŒ–å»ºè®®"""
        # å‡†å¤‡ä¸Šä¸‹æ–‡
        context = "## çƒ­ç‚¹å‡½æ•°åˆ†æ\n\n"
        
        for hotspot in hotspots[:3]:
            func_name = hotspot["function"]
            func_info = next((f for f in functions if f["name"] == func_name), None)
            
            if func_info:
                context += f"### {func_name} [{hotspot['severity']}]\n"
                context += f"ä½ç½®: {hotspot['file']}:{hotspot['lines']}\n"
                context += f"æ ¹æœ¬åŸå› : {hotspot['root_cause']}\n"
                context += f"```{language}\n{func_info.get('code_snippet', '')[:1000]}\n```\n\n"

        profiling_data = state.get("profiling_data")
        if profiling_data and profiling_data.get("hotspots"):
            context += "## åŠ¨æ€å‰–ææ‘˜è¦\n\n"
            context += f"- æ€»è€—æ—¶: {profiling_data.get('total_time', 'N/A')}\n"
            for spot in profiling_data.get("hotspots", [])[:5]:
                context += f"- {spot.get('function', '')}: {spot.get('percent', '')}\n"
            context += "\n"
        
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªé«˜çº§æ€§èƒ½ä¼˜åŒ–ä¸“å®¶ã€‚åŸºäºä»¥ä¸‹åˆ†æç»“æœï¼Œä¸ºæ¯ä¸ªçƒ­ç‚¹å‡½æ•°æä¾›å…·ä½“çš„ä¼˜åŒ–æ–¹æ¡ˆã€‚

{context}

å¯¹äºæ¯ä¸ªçƒ­ç‚¹ï¼Œè¯·æä¾›ï¼š
1. å…·ä½“çš„ä¼˜åŒ–æ–¹æ¡ˆï¼ˆä¸æ˜¯æ³›æ³›çš„å»ºè®®ï¼‰
2. ä¼˜åŒ–å‰çš„ä»£ç ç¤ºä¾‹
3. ä¼˜åŒ–åçš„ä»£ç ç¤ºä¾‹
4. é¢„æœŸçš„æ€§èƒ½æå‡

æ³¨æ„ï¼šä¸è¦è¾“å‡ºâ€œç®—æ³•åç§°/æ—¶é—´å¤æ‚åº¦æ¨å¯¼â€ã€‚åªå…³æ³¨å¯è½åœ°çš„æ€§èƒ½ä¼˜åŒ–ï¼ˆå‡å°‘ CPU æŒ‡ä»¤æ•°ã€å‡å°‘å†…å­˜è®¿é—®/åˆ†é…ã€æ”¹è¿› I/Oã€å¹¶è¡ŒåŒ–ã€ç¼“å­˜ç­‰ï¼‰ã€‚

è¾“å‡º JSON æ ¼å¼ï¼š
[
  {{
    "target": "å‡½æ•°å",
    "priority": "high/medium/low",
    "category": "algorithm/data_structure/memory/parallelization/cache",
    "problem": "é—®é¢˜æè¿°",
    "solution": "è¯¦ç»†çš„è§£å†³æ–¹æ¡ˆ",
    "code_before": "ä¼˜åŒ–å‰ä»£ç ",
    "code_after": "ä¼˜åŒ–åä»£ç ",
    "expected_improvement": "é¢„æœŸæå‡"
  }}
]
"""
        
        try:
            response = self.llm.invoke([HumanMessage(content=prompt)])
            content = response.content
            
            import json
            import re
            json_match = re.search(r'\[[\s\S]*\]', content)
            if json_match:
                suggestions_data = json.loads(json_match.group())
                
                suggestions = []
                for s in suggestions_data:
                    suggestions.append(OptimizationSuggestion(
                        target=s.get("target", ""),
                        priority=s.get("priority", "medium"),
                        category=s.get("category", "other"),
                        problem=s.get("problem", ""),
                        solution=s.get("solution", ""),
                        code_before=s.get("code_before", ""),
                        code_after=s.get("code_after", ""),
                        expected_improvement=s.get("expected_improvement", "")
                    ))
                
                return suggestions
                
        except Exception as e:
            logger.warning(f"LLM ä¼˜åŒ–åˆ†æå¤±è´¥: {e}")
        
        return []
    
    def _deduplicate_and_prioritize(self, 
                                     suggestions: List[OptimizationSuggestion]) -> List[OptimizationSuggestion]:
        """å»é‡å¹¶æŒ‰ä¼˜å…ˆçº§æ’åº"""
        seen = set()
        unique = []
        
        for s in suggestions:
            key = (s["target"], s["category"])
            if key not in seen:
                seen.add(key)
                unique.append(s)
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        priority_order = {"high": 0, "medium": 1, "low": 2}
        unique.sort(key=lambda x: priority_order.get(x["priority"], 3))
        
        return unique
    
    def _generate_performance_report(self, state: PerformanceState,
                                      optimizations: List[OptimizationSuggestion]) -> str:
        """ç”Ÿæˆå®Œæ•´çš„æ€§èƒ½åˆ†ææŠ¥å‘Š"""
        report = "# æ€§èƒ½åˆ†ææŠ¥å‘Š\n\n"
        
        # é¡¹ç›®æ¦‚è§ˆ
        report += "## 1. é¡¹ç›®æ¦‚è§ˆ\n\n"
        report += f"- é¡¹ç›®è·¯å¾„: `{state['project_path']}`\n"
        report += f"- è¯­è¨€: {state.get('language', 'C')}\n"
        report += f"- åˆ†æå‡½æ•°æ•°: {len(state.get('functions', []))}\n"
        report += f"- åŠ¨æ€å‰–æ: {'å¯ç”¨' if state.get('profiling_enabled') else 'æœªå¯ç”¨'}\n\n"

        # åŠ¨æ€å‰–ææ‘˜è¦
        profiling_data = state.get("profiling_data")
        if profiling_data:
            report += "## 2. åŠ¨æ€å‰–æè§£è¯»\n\n"
            report += f"- æ€»è€—æ—¶: {profiling_data.get('total_time', 'N/A')}\n"
            report += f"- å†…å­˜å³°å€¼: {profiling_data.get('memory_peak', 'N/A')}\n"

            cache_info = profiling_data.get("cache_info") or {}
            if cache_info:
                # åªæŒ‘å…³é”®æŒ‡æ ‡å±•ç¤º
                if cache_info.get("cpu_percent"):
                    report += f"- CPU ä½¿ç”¨ç‡: {cache_info.get('cpu_percent')}\n"
                if cache_info.get("user_time_s") or cache_info.get("system_time_s"):
                    report += (
                        f"- CPU æ—¶é—´: user={cache_info.get('user_time_s', 'N/A')}s, "
                        f"sys={cache_info.get('system_time_s', 'N/A')}s\n"
                    )
                if cache_info.get("major_page_faults") or cache_info.get("minor_page_faults"):
                    report += (
                        f"- é¡µé”™è¯¯: major={cache_info.get('major_page_faults', 'N/A')}, "
                        f"minor={cache_info.get('minor_page_faults', 'N/A')}\n"
                    )
                if cache_info.get("voluntary_ctx_switches") or cache_info.get("involuntary_ctx_switches"):
                    report += (
                        f"- ä¸Šä¸‹æ–‡åˆ‡æ¢: voluntary={cache_info.get('voluntary_ctx_switches', 'N/A')}, "
                        f"involuntary={cache_info.get('involuntary_ctx_switches', 'N/A')}\n"
                    )
                if cache_info.get("fs_inputs") or cache_info.get("fs_outputs"):
                    report += (
                        f"- æ–‡ä»¶ç³»ç»Ÿ I/O: in={cache_info.get('fs_inputs', 'N/A')}, "
                        f"out={cache_info.get('fs_outputs', 'N/A')}\n"
                    )

            # ç®€å•ç»“è®ºï¼ˆä¸å±•ç¤ºåŸå§‹è¾“å‡ºï¼‰
            cpu_percent = (cache_info.get("cpu_percent") or "").strip()
            if cpu_percent.endswith("%"):
                try:
                    cpu_val = int(cpu_percent[:-1])
                    if cpu_val >= 90:
                        report += "\n**åˆæ­¥åˆ¤æ–­**: CPU ç»‘å®šè¾ƒæ˜æ˜¾ï¼Œä¼˜å…ˆå…³æ³¨çƒ­ç‚¹å‡½æ•°ä¸ç®—æ³•/æ•°æ®ç»“æ„å±‚é¢çš„ä¼˜åŒ–ã€‚\n\n"
                    elif cpu_val <= 40:
                        report += "\n**åˆæ­¥åˆ¤æ–­**: å¯èƒ½å­˜åœ¨ I/O ç»‘å®šæˆ–ç­‰å¾…ï¼ˆCPU åˆ©ç”¨ç‡åä½ï¼‰ï¼Œä¼˜å…ˆæ£€æŸ¥ç£ç›˜è®¿é—®/ç³»ç»Ÿè°ƒç”¨/é”ç­‰å¾…ã€‚\n\n"
                except Exception:
                    pass
        
        # æ€§èƒ½çƒ­ç‚¹
        hotspots = state.get("hotspots", [])
        if hotspots:
            report += "## 3. æ€§èƒ½çƒ­ç‚¹\n\n"
            for spot in hotspots[:5]:
                report += f"### ğŸ”¥ #{spot['rank']} {spot['function']} [{spot['severity']}]\n\n"
                report += f"- **ä½ç½®**: `{spot['file']}:{spot['lines']}`\n"
                report += f"- **æ ¹æœ¬åŸå› **: {spot['root_cause']}\n\n"
        
        # å†…å­˜é—®é¢˜
        memory_issues = state.get("memory_issues", [])
        if memory_issues:
            report += "## 4. å†…å­˜é—®é¢˜\n\n"
            high_issues = [i for i in memory_issues if i['severity'] == 'high']
            if high_issues:
                report += f"âš ï¸ å‘ç° **{len(high_issues)}** ä¸ªé«˜ä¸¥é‡æ€§å†…å­˜é—®é¢˜\n\n"
            for issue in memory_issues[:5]:
                icon = "ğŸ”´" if issue['severity'] == 'high' else "ğŸŸ¡" if issue['severity'] == 'medium' else "ğŸŸ¢"
                report += f"{icon} **{issue['type']}** ({issue['file']}:{issue['line']})\n"
                report += f"   {issue['description']}\n\n"
        
        # ä¼˜åŒ–å»ºè®®
        if optimizations:
            report += "## 5. ä¼˜åŒ–å»ºè®®\n\n"
            for i, opt in enumerate(optimizations[:10], 1):
                priority_icon = "ğŸ”´" if opt['priority'] == 'high' else "ğŸŸ¡" if opt['priority'] == 'medium' else "ğŸŸ¢"
                report += f"### {priority_icon} å»ºè®® {i}: {opt['target']}\n\n"
                report += f"**ç±»åˆ«**: {opt['category']} | **ä¼˜å…ˆçº§**: {opt['priority']}\n\n"
                report += f"**é—®é¢˜**: {opt['problem']}\n\n"
                report += f"**è§£å†³æ–¹æ¡ˆ**: {opt['solution']}\n\n"
                
                if opt.get('code_before') and opt.get('code_after'):
                    report += "**ä»£ç ç¤ºä¾‹**:\n\n"
                    report += f"ä¼˜åŒ–å‰:\n```c\n{opt['code_before']}\n```\n\n"
                    report += f"ä¼˜åŒ–å:\n```c\n{opt['code_after']}\n```\n\n"
                
                report += f"**é¢„æœŸæå‡**: {opt['expected_improvement']}\n\n"
                report += "---\n\n"
        
        # æ€»ç»“
        report += "## 6. æ€»ç»“\n\n"
        high_priority = len([o for o in optimizations if o['priority'] == 'high'])
        report += f"- å‘ç° **{len(hotspots)}** ä¸ªæ€§èƒ½çƒ­ç‚¹\n"
        report += f"- å‘ç° **{len(memory_issues)}** ä¸ªå†…å­˜é—®é¢˜\n"
        report += f"- ç”Ÿæˆ **{len(optimizations)}** æ¡ä¼˜åŒ–å»ºè®®\n"
        report += f"- å…¶ä¸­ **{high_priority}** æ¡ä¸ºé«˜ä¼˜å…ˆçº§\n"
        
        return report
